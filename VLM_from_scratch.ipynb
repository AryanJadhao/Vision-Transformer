{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlLPfYpUl7MlGqjjNd3MOm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryanJadhao/Vision-Transformer/blob/main/VLM_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "atyRmHMpmp4Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "t5u1pxjymjdP"
      },
      "outputs": [],
      "source": [
        "import math, random\n",
        "import torch\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables"
      ],
      "metadata": {
        "id": "vdVpcOZvnLbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda import temperature\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "img_size = 32\n",
        "embed_size = 64\n",
        "attention_heads = 4\n",
        "batch_size = 12\n",
        "Temperature = 0.07\n",
        "epochs = 20\n",
        "Lr = 3e-4"
      ],
      "metadata": {
        "id": "-Ve2p_9bnKxs"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthetic Dataset Properties"
      ],
      "metadata": {
        "id": "E5iqDl_0n0QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors= ['red','green','blue','yellow','purple','orange','pink','brown','gray']\n",
        "shapes = ['square','circle','triangle']\n",
        "positions = ['left','center','right','top','bottom','top-left','top-right','bottom-left','bottom-right']"
      ],
      "metadata": {
        "id": "_cplyZkWnxke"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drawing Image Shapes"
      ],
      "metadata": {
        "id": "VqD16PW7ob8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_sample(color, shapes, positions, img_size=img_size):\n",
        "  img = Image.new('RGB', (img_size, img_size), 'white')\n",
        "  draw = ImageDraw.Draw(img)\n",
        "\n",
        "  margin = 6\n",
        "  h = w = img_size - 2*margin\n",
        "\n",
        "  # Calculate x coordinates\n",
        "  if 'left' in positions:\n",
        "    x0 = margin\n",
        "    x1 = margin + w // 2\n",
        "  elif 'top-left' in positions:\n",
        "    x0 = margin\n",
        "    x1 = margin + w // 2\n",
        "  elif 'bottom-left' in positions:\n",
        "    x0 = margin\n",
        "    x1 = margin + w // 2\n",
        "  elif 'right' in positions:\n",
        "    x0 = margin + w // 2\n",
        "    x1 = img_size - margin\n",
        "  elif 'top-right' in positions:\n",
        "    x0 = margin + w // 2\n",
        "    x1 = img_size - margin\n",
        "  elif 'bottom-right' in positions:\n",
        "    x0 = margin + w // 2\n",
        "    x1 = img_size - margin\n",
        "  else:\n",
        "    x0 = margin + w // 4\n",
        "    x1 = margin + 3*w // 4\n",
        "\n",
        "  # calculate y coordinates\n",
        "  if 'top' in positions:\n",
        "    y0 = margin\n",
        "    y1 = margin + h // 2\n",
        "  elif 'top-left' in positions:\n",
        "    y0 = margin\n",
        "    y1 = margin + h // 2\n",
        "  elif 'top-right' in positions:\n",
        "    y0 = margin\n",
        "    y1 = margin + h // 2\n",
        "  elif 'bottom' in positions:\n",
        "    y0 = margin + h // 2\n",
        "    y1 = img_size - margin\n",
        "  elif 'bottom-left' in positions:\n",
        "    y0 = margin + h // 2\n",
        "    y1 = img_size - margin\n",
        "  elif 'bottom-right' in positions:\n",
        "    y0 = margin + h // 2\n",
        "    y1 = img_size - margin\n",
        "  else:\n",
        "    y0 = margin + h // 4\n",
        "    y1 = margin + 3*h // 4\n",
        "\n",
        "  if shapes == 'square':\n",
        "    draw.rectangle([x0,y0,x1,y1], fill=color, outline='black')\n",
        "  elif shapes == 'circle':\n",
        "    draw.ellipse([x0,y0,x1,y1], fill=color, outline='black')\n",
        "  else:\n",
        "    draw.polygon([(x0+(x1-x0)//2, y0),(x0,y1),(x1,y1)], fill=color, outline='black')\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "wAOEkByQoaro"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## class for building our dataset"
      ],
      "metadata": {
        "id": "Gp1rD40guHqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "fR2k-NQev49V"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class shapesDataset():\n",
        "  def __init__(self):\n",
        "    self.images = []\n",
        "    self.captions = []\n",
        "\n",
        "    for c in colors:\n",
        "      for s in shapes:\n",
        "        for p in positions:\n",
        "          img = draw_sample(c,s,p)\n",
        "          cap = f\"{c} {s} {p}\"\n",
        "\n",
        "          self.images.append(torch.from_numpy(np.asarray(img)).permute(2,0,1).float()/255.0)\n",
        "          self.captions.append(cap)\n",
        "\n",
        "    self.vocab, self.word2idx = self.build_vocab(self.captions)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def build_vocab(self, texts):\n",
        "      words = sorted({w for t in texts for w in t.split()})\n",
        "      vocab = ['[CLS]'] + words\n",
        "      w2i = {w:i for i,w in enumerate(vocab)}\n",
        "\n",
        "      return vocab, w2i\n",
        "\n",
        "  def encode_text(self,text):\n",
        "      toks = [self.word2idx['[CLS]']] + [self.word2idx[w] for w in text.split()]\n",
        "      return torch.tensor(toks, dtype=torch.long)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "      return self.images[idx], self.encode_text(self.captions[idx])"
      ],
      "metadata": {
        "id": "G5Mm1mSZuD6X"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_ds = shapesDataset()\n",
        "vocab_size = len(full_ds.vocab)\n",
        "print(vocab_size)\n",
        "print(full_ds.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpAbtxmqzRak",
        "outputId": "8dd179db-a769-40c4-9d1e-68bfb13ac60f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n",
            "['[CLS]', 'blue', 'bottom', 'bottom-left', 'bottom-right', 'brown', 'center', 'circle', 'gray', 'green', 'left', 'orange', 'pink', 'purple', 'red', 'right', 'square', 'top', 'top-left', 'top-right', 'triangle', 'yellow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train-Val data creation"
      ],
      "metadata": {
        "id": "aNqqR7EB0kDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(full_ds))\n",
        "val_size = len(full_ds) - train_size\n",
        "train_ds, val_ds = torch.utils.data.random_split(full_ds, [train_size, val_size])\n"
      ],
      "metadata": {
        "id": "0xEf_9L8zZQE"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DataLoader"
      ],
      "metadata": {
        "id": "HC9rj_fG2BDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_ds, batch_size = batch_size, shuffle=True)\n",
        "val_loader =  DataLoader(val_ds, batch_size = batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "d7L144or05go"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Sample Images"
      ],
      "metadata": {
        "id": "Fw_AH45y2YXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, encoded_caps = next(iter(train_loader))\n",
        "idx = random.randint(0, len(imgs)-1)\n",
        "img = (imgs[idx].permute(1,2,0).numpy()*255).astype(np.uint8)  #convert to displayable images\n",
        "\n",
        "# decode the captions\n",
        "captions_tokens = encoded_caps[idx].tolist()\n",
        "caption = \" \".join([full_ds.vocab[i] for i in captions_tokens if i in range(len(full_ds.vocab))])\n",
        "caption = caption.replace(\"[CLS]\",\"\")\n",
        "\n",
        "plt.figure(figsize=(2.5,2.5))\n",
        "plt.imshow(img)\n",
        "plt.title(caption, fontsize=8)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "KOwQBu6d2TS-",
        "outputId": "c53ee5ea-90ff-46d2-ecf9-759012f33e82"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 250x250 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image Encoder"
      ],
      "metadata": {
        "id": "PT165kZy6qSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "  def __init__(self, embed_dim=embed_size):\n",
        "    super().__init__()\n",
        "    self.convolutions = nn.Sequential(\n",
        "        nn.Conv2d(3,32,3,2,1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32,64,3,2,1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,128,3,2,1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128,256,3,2,1),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.projection = nn.Linear(256, embed_dim)\n",
        "    self.layernorm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convolutions(x)\n",
        "    x = x.mean(dim=[2,3])\n",
        "    x = self.projection(x)\n",
        "    x = F.normalize(self.layernorm1(x), dim=-1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "U6Dv5k573A37"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Encoder"
      ],
      "metadata": {
        "id": "CqmDn_J28_wF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim=embed_size, num_heads =attention_heads, context_window=4):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.position_embedding = nn.Embedding(context_window, embed_dim)\n",
        "    self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "    self.projection = nn.Linear(embed_dim, embed_dim)\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, toks):\n",
        "    N,L = toks.shape\n",
        "    position_emb_ids = torch.arange(L, device=toks.device).unsqueeze(0).expand(N,L)\n",
        "    position_embedding_vectors = self.position_embedding(position_emb_ids)\n",
        "    token_emb_ids = toks\n",
        "    token_embedding_vectors = self.token_embedding(token_emb_ids)\n",
        "\n",
        "    x = token_embedding_vectors + position_embedding_vectors\n",
        "    x = self.mha(x, x, x)[0]\n",
        "    x = x[:,0]\n",
        "    x = self.projection(x)\n",
        "    x = F.normalize(self.norm(x), dim=-1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ifDoDDkA841R"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CLIP loss"
      ],
      "metadata": {
        "id": "TIaLZKEJDcqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_loss(img_emb, txt_emb, temperature=Temperature):\n",
        "  logits = (img_emb @ txt_emb.T) / temperature\n",
        "  targets = torch.arange(img_emb.size(0), device=img_emb.device)\n",
        "  loss_i = F.cross_entropy(logits, targets)\n",
        "  loss_t = F.cross_entropy(logits.T, targets)\n",
        "  return ((loss_i + loss_t)/2.0)"
      ],
      "metadata": {
        "id": "i4Ty0Cs3DY-0"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model, data, optimizer"
      ],
      "metadata": {
        "id": "e82U8keUE3EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_enc = ImageEncoder().to(device)\n",
        "txt_enc = TextEncoder(vocab_size).to(device)\n",
        "params = list(img_enc.parameters()) + list(txt_enc.parameters())\n",
        "optimizer = torch.optim.AdamW(params, lr=Lr)"
      ],
      "metadata": {
        "id": "e4Gom3xDExpW"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "best_val = float('inf')\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "  img_enc.train()\n",
        "  txt_enc.train()\n",
        "\n",
        "  total = 0.0\n",
        "\n",
        "  for imgs, toks in train_loader:\n",
        "    imgs = imgs.to(device)\n",
        "    toks = toks.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    img_emb = img_enc(imgs)\n",
        "    txt_emb = txt_enc(toks)\n",
        "    loss = clip_loss(img_emb, txt_emb)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total += loss.item()*imgs.size(0)\n",
        "  train_loss = total/(len(train_loader)*batch_size)\n",
        "\n",
        "  # quick val\n",
        "  img_enc.eval()\n",
        "  txt_enc.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    vtotal, n = 0.0, 0\n",
        "    for imgs, toks in val_loader:\n",
        "      imgs = imgs.to(device)\n",
        "      toks = toks.to(device)\n",
        "\n",
        "      vtotal += clip_loss(img_enc(imgs), txt_enc(toks)).item()*imgs.size(0)\n",
        "\n",
        "      n += imgs.size(0)\n",
        "    val_loss = vtotal/n\n",
        "\n",
        "  print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "  if val_loss < best_val:\n",
        "    best_val = val_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHpgNifKH7MG",
        "outputId": "790b13cc-c9ec-4bde-88f2-0d10abe847ed"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 2.3598 | Val Loss: 2.4219\n",
            "Epoch 02 | Train Loss: 2.2948 | Val Loss: 2.1569\n",
            "Epoch 03 | Train Loss: 1.5853 | Val Loss: 1.4274\n",
            "Epoch 04 | Train Loss: 1.0213 | Val Loss: 0.9014\n",
            "Epoch 05 | Train Loss: 0.6972 | Val Loss: 0.5711\n",
            "Epoch 06 | Train Loss: 0.5707 | Val Loss: 0.4331\n",
            "Epoch 07 | Train Loss: 0.5020 | Val Loss: 0.5788\n",
            "Epoch 08 | Train Loss: 0.4210 | Val Loss: 0.3337\n",
            "Epoch 09 | Train Loss: 0.2961 | Val Loss: 0.2820\n",
            "Epoch 10 | Train Loss: 0.1873 | Val Loss: 0.2670\n",
            "Epoch 11 | Train Loss: 0.1769 | Val Loss: 0.1770\n",
            "Epoch 12 | Train Loss: 0.1554 | Val Loss: 0.1712\n",
            "Epoch 13 | Train Loss: 0.1295 | Val Loss: 0.1730\n",
            "Epoch 14 | Train Loss: 0.1048 | Val Loss: 0.1675\n",
            "Epoch 15 | Train Loss: 0.1371 | Val Loss: 0.2529\n",
            "Epoch 16 | Train Loss: 0.1330 | Val Loss: 0.2739\n",
            "Epoch 17 | Train Loss: 0.1419 | Val Loss: 0.1371\n",
            "Epoch 18 | Train Loss: 0.1020 | Val Loss: 0.1014\n",
            "Epoch 19 | Train Loss: 0.0723 | Val Loss: 0.0927\n",
            "Epoch 20 | Train Loss: 0.0555 | Val Loss: 0.1241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLwJJNHWRFb5"
      },
      "execution_count": 92,
      "outputs": []
    }
  ]
}